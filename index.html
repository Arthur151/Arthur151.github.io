<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yu Sun</title>
  
  <meta name="author" content="Yu Sun">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yu Sun</name>
              </p>
              <p>I am a Machine Learning Scientist at <a href="https://meshcapade.com">Meshcapade</a>. Previously, I was a doctoral student at Harbin Institute of Technology (HIT) and an intern at JDAI CV lab in China. 
              </p>
              <p>
                I am focusing on the research of monocular 3D human motion estimation. 
                At Meshcapade, I work with many talents in <a href="https://meshcapade.com/team">ML team</a>, which is led by 
                <a href="https://scholar.google.com/citations?user=u_RQBcUAAAAJ&hl=en">Naureen Mahmood</a>,
                <a href="https://www.linkedin.com/in/talha-zaman/">Talha Zaman</a>,
                <a href="https://ps.is.tuebingen.mpg.de/person/black">Michael J. Black</a>,
                and <a href="https://www.linkedin.com/in/nicolasheron/">Nicolas Heron</a>.
              </p>
              <p>
                At HIT, I was supervised by <a href="http://homepage.hit.edu.cn/wpgao">Wenpeng Gao</a> 
                and <a href="http://homepage.hit.edu.cn/fuyili">Yili Fu</a>.
                At JDAI, I was supervised by 
                <a href="https://scholar.google.com/citations?user=vhM_c14AAAAJ&hl=zh-CN">Qian Bao</a>, 
                <a href="https://drliuwu.com">Wu Liu</a>, 
                <a href="https://scholar.google.com/citations?hl=en&user=wxvX51gAAAAJ">Yun Ye</a>, 
                Xiaolei Lv, 
                and <a href="https://taomei.me/">Tao Mei</a>.
                In exploring this field, it's an honor to work with 
                <a href="https://ps.is.tuebingen.mpg.de/person/black">Michael J. Black</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:yusunhit@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=fkGxgrsAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/yusun14567741">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/Arthur151/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/selfie.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/selfie.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				

<!-- <tr onmouseout="trace_stop()" onmouseover="trace_start()"  bgcolor="#ffffd0"> -->
  <tr onmouseout="trace_stop()" onmouseover="trace_start()">
    <td class="paper_tb">
      <div class="tb_one">
        <div class="tb_two">
          <img src="TRACE/images/demo.gif">
        </div>
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://arthur151.github.io/TRACE/TRACE.html">
        <papertitle>TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras in 3D Environments</papertitle>
      </a>
      <br>
      <strong>Yu Sun</strong>,
      <a href="https://scholar.google.com/citations?user=vhM_c14AAAAJ&hl=zh-CN">Qian Bao</a>,
      <a href="https://drliuwu.com">Wu Liu</a>,
      <a href="https://taomei.me/">Tao Mei</a>,
      <a href="https://ps.is.mpg.de/~black">Michael J. Black</a>
      <br>
      <em>CVPR</em>, 2023 &nbsp
      <br>
      <a href="https://arthur151.github.io/TRACE/TRACE.html">project page</a>
      /
      <a href="https://arxiv.org/abs/2306.02850">arXiv</a>
      /
      <a href="https://github.com/Arthur151/ROMP">code</a>
      /
      <a href="https://github.com/Arthur151/DynaCam">dataset</a>
      /
      <a href="https://www.youtube.com/embed/l8aLHDXWQRw">video</a>
      /
      <a href="references/trace.bib">bib</a>
      <p></p>
      <p>
      With a holistic 5D representation, TRACE tracks the subjects presented in the first frame through time and recovers their 3D trajectories in global coordinates. It does so in one shot. 
      </p>
    </td>
  </tr>

<!-- <tr onmouseout="bev_stop()" onmouseover="bev_start()"  bgcolor="#ffffd0"> -->
<tr onmouseout="bev_stop()" onmouseover="bev_start()">
  <td class="paper_tb">
    <div class="tb_one">
      <div class="tb_two">
        <img src="BEV/images/crowd3.png">
      </div>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arthur151.github.io/BEV/BEV.html">
      <papertitle>Putting People in their Place: Monocular Regression of 3D People in Depth</papertitle>
    </a>
    <br>
    <strong>Yu Sun</strong>,
    <a href="https://drliuwu.com">Wu Liu</a>,
    <a href="https://scholar.google.com/citations?user=vhM_c14AAAAJ&hl=zh-CN">Qian Bao</a>,
    <a href="http://homepage.hit.edu.cn/fuyili">Yili Fu</a>,
    <a href="https://taomei.me/">Tao Mei</a>,
    <a href="https://ps.is.mpg.de/~black">Michael J. Black</a>
    <br>
    <em>CVPR</em>, 2022 &nbsp
    <br>
    <a href="https://arthur151.github.io/BEV/BEV.html">project page</a>
    /
    <a href="https://arxiv.org/abs/2112.08274">arXiv</a>
    /
    <a href="https://github.com/Arthur151/ROMP">code</a>
    /
    <a href="https://github.com/Arthur151/Relative_Human">dataset</a>
    /
    <a href="https://www.youtube.com/embed/Q62fj_6AxRI">video</a>
    /
    <a href="references/BEV.bib">bib</a>
    <p></p>
    <p>
    BEV adopts an imaginary Bird's-Eye-View representation to explicitly reason about depth relationships between people.
    </p>
  </td>
</tr>


<tr onmouseout="romp_stop()" onmouseover="romp_start()">
  <td class="paper_tb">
    <div class="tb_one">
      <div class="tb_two">
        <img class="img-fluid" src="images/romp_blender_character_driven.gif">
      </div>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://github.com/Arthur151/ROMP">
      <papertitle>Monocular, One-stage, Regression of Multiple 3D People</papertitle>
    </a>
    <br>
    <strong>Yu Sun</strong>,
    <a href="https://scholar.google.com/citations?user=vhM_c14AAAAJ&hl=zh-CN">Qian Bao</a>,
    <a href="https://drliuwu.com">Wu Liu</a>,
    <a href="http://homepage.hit.edu.cn/fuyili">Yili Fu</a>,
    <a href="https://ps.is.mpg.de/~black">Michael J. Black</a>,
    <a href="https://taomei.me/">Tao Mei</a>
    <br>
    <em>ICCV</em>, 2021 &nbsp
    <br>
    <a href="https://arxiv.org/abs/2008.12272">arXiv</a>
    /
    <a href="https://github.com/Arthur151/ROMP">code</a>
    /
    <a href="https://www.youtube.com/watch?v=hunBPJxnyBU">video</a>
    /
    <a href="references/ROMP.bib">bib</a>
    <p></p>
    <p>
    ROMP is the first one-stage method for monocular regression of multiple 3D people. It can run at over 20 FPS on a 1070Ti GPU.
    </p>
  </td>
</tr>

<tr onmouseout="cbd_stop()" onmouseover="cbd_start()">
  <td class="paper_tb">
    <div class="tb_one">
      <div class="tb_two">
        <img src="https://ieeexplore.ieee.org/ielx7/6046/10384483/10179931/graphical_abstract/tmm-gagraphic-3294820.jpg">
      </div>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://ieeexplore.ieee.org/abstract/document/10179931">
      <papertitle>Learning Monocular Regression of 3D People in Crowds via Scene-Aware Blending and De-Occlusion</papertitle>
    </a>
    <br>
    <strong>Yu Sun</strong>,
    Lubing Xu,
    <a href="https://scholar.google.com/citations?user=vhM_c14AAAAJ&hl=zh-CN">Qian Bao</a>,
    <a href="https://drliuwu.com">Wu Liu</a>,
    <a href="http://homepage.hit.edu.cn/wpgao">Wenpeng Gao</a>,
    <a href="http://homepage.hit.edu.cn/fuyili">Yili Fu</a>,
    <br>
    <em>T-MM</em>, 2023 &nbsp
    <br>
    <a href="https://ieeexplore.ieee.org/abstract/document/10179931">paper</a>
    /
    <a href="references/cbd.bib">bib</a>
    <p></p>
  </td>
</tr>

<tr onmouseout="dsd_stop()" onmouseover="dsd_start()">
  <td class="paper_tb">
    <div class="tb_one">
      <div class="tb_two">
        <img src="images/icassp.png">
      </div>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://ieeexplore.ieee.org/abstract/document/9747426">
      <papertitle>Learning Monocular Mesh Recovery of Multiple Body Parts Via Synthesis</papertitle>
    </a>
    <br>
    <strong>Yu Sun</strong>,
    <a href="https://www.linkedin.com/in/tianyu-huang-645a91193?trk=public_profile_samename_mini-profile_title">Tianyu Huang</a>,
    <a href="https://scholar.google.com/citations?user=vhM_c14AAAAJ&hl=zh-CN">Qian Bao</a>,
    <a href="https://drliuwu.com">Wu Liu</a>,
    <a href="http://homepage.hit.edu.cn/wpgao">Wenpeng Gao</a>,
    <a href="http://homepage.hit.edu.cn/fuyili">Yili Fu</a>,
    <br>
    <em>ICASSP</em>, 2022 &nbsp
    <br>
    <a href="https://ieeexplore.ieee.org/abstract/document/97474262">paper</a>
    /
    <a href="references/icassp22.bib">bib</a>
    <p></p>
  </td>
</tr>

<tr onmouseout="dsd_stop()" onmouseover="dsd_start()">
  <td class="paper_tb">
    <div class="tb_one">
      <div class="tb_two">
        <img src="images/dsd_satn_demos.png">
      </div>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/1908.07172">
      <papertitle>Human Mesh Recovery from Monocular Images via a Skeleton-disentangled Representation</papertitle>
    </a>
    <br>
    <strong>Yu Sun</strong>,
    <a href="https://scholar.google.com/citations?hl=en&user=wxvX51gAAAAJ">Yun Ye</a>,
    <a href="https://drliuwu.com">Wu Liu</a>,
    <a href="http://homepage.hit.edu.cn/wpgao">Wenpeng Gao</a>,
    <a href="http://homepage.hit.edu.cn/fuyili">Yili Fu</a>,
    <a href="https://taomei.me/">Tao Mei</a>
    <br>
    <em>ICCV</em>, 2019 &nbsp
    <br>
    <a href="https://arxiv.org/abs/1908.07172">arXiv</a>
    /
    <a href="https://github.com/JDAI-CV/DSD-SATN">code</a>
    /
    <a href="https://www.youtube.com/watch?v=GG-8If4uVQM">video results</a>
    /
    <a href="references/DSD-STAN.bib">bib</a>
    <p></p>
    <p>
    DSD-SATN attempts to disentangle the pose-related features from the others (e.g. shape, camera) and learn temporal dynamics via sorting shuffled frames.
    </p>
  </td>
</tr>





</tbody></table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
  <tr>
    <td>
      <heading>Collaborative Research</heading>
    </td>
  </tr>
</tbody></table>
<table width="100%" align="center" border="0" cellpadding="20"><tbody>


<!-- <tr onmouseout="bev_stop()" onmouseover="bev_start()"  bgcolor="#ffffd0"> -->
<tr onmouseout="phmr_stop()" onmouseover="phmr_start()">
  <td class="paper_tb">
    <div class="tb_one">
      <div class="tb_two">
        <img src="images/prompthmr_teaser.jpg">
      </div>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://yufu-wang.github.io/phmr-page/">
      <papertitle>PromptHMR: Promptable Human Mesh Recovery</papertitle>
    </a>
    <br>
    <a href="https://yufu-wang.github.io/">Yufu Wang</a>,
    <strong>Yu Sun</strong>,
    <a href="https://pixelite1201.github.io/">Priyanka Patel</a>,
    <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>,
    <a href="https://ps.is.mpg.de/~black">Michael J. Black</a>,
    <a href="https://is.mpg.de/ps/person/mkocabas">Muhammed Kocabas</a>
    <br>
    <em>CVPR</em>, 2025 &nbsp
    <br>
    <a href="https://yufu-wang.github.io/phmr-page/">project page</a>
    /
    <a href="https://arxiv.org/abs/2504.06397">arXiv</a>
    /
    <a href="https://github.com/yufu-wang/PromptHMR">code</a>
    /
    <a href="https://www.youtube.com/embed/Q62fj_6AxRI">video</a>
    <p></p>
    <p>
    PromptHMR adopts multi-modal prompts, e.g. 2D detection, segmentation, text, etc, to promote the HPS estimation.
    </p>
  </td>
</tr>

<!-- <tr onmouseout="bev_stop()" onmouseover="bev_start()"  bgcolor="#ffffd0"> -->
<tr onmouseout="thmr_stop()" onmouseover="thmr_start()">
  <td class="paper_tb">
    <div class="tb_one">
      <div class="tb_two">
        <img src="https://tokenhmr.is.tue.mpg.de/media/upload/tokenhmr_method.png">
      </div>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://tokenhmr.is.tue.mpg.de/">
      <papertitle>TokenHMR: Advancing Human Mesh Recovery with a Tokenized Pose Representation</papertitle>
    </a>
    <br>
    <a href="https://saidwivedi.in/">Sai Kumar Dwivedi*</a>,
    <strong>Yu Sun*</strong>,
    <a href="https://pixelite1201.github.io/">Priyanka Patel</a>,
    <a href="https://yfeng95.github.io/">Yao Feng</a>,
    <a href="https://ps.is.mpg.de/~black">Michael J. Black</a>
    <br>
    <em>CVPR</em>, 2024 &nbsp
    <br>
    <a href="https://tokenhmr.is.tue.mpg.de/">project page</a>
    /
    <a href="https://arxiv.org/abs/2404.16752">arXiv</a>
    /
    <a href="https://github.com/saidwivedi/TokenHMR">code</a>
    /
    <a href="https://www.youtube.com/watch?v=Jgg_LfvoE4A">video</a>
    <p></p>
    <p>
    TokenHMR adopts a discrete-token-based representation for SMPL-based regression.
    </p>
  </td>
</tr>


<!-- <tr onmouseout="bev_stop()" onmouseover="bev_start()"  bgcolor="#ffffd0"> -->
<tr onmouseout="chatpose_stop()" onmouseover="chatpose_start()">
  <td class="paper_tb">
    <div class="tb_one">
      <div class="tb_two">
        <img src="https://yfeng95.github.io/ChatPose/static/figures/method.png">
      </div>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://yfeng95.github.io/ChatPose/">
      <papertitle>ChatPose: Chatting about 3D Human Pose</papertitle>
    </a>
    <br>
    <a href="https://yfeng95.github.io/">Yao Feng</a>,
    <a href="https://jinglin7.github.io/">Jing Lin</a>,
    <a href="https://saidwivedi.in/">Sai Kumar Dwivedi</a>,
    <strong>Yu Sun</strong>,
    <a href="https://pixelite1201.github.io/">Priyanka Patel</a>,
    <a href="https://ps.is.mpg.de/~black">Michael J. Black</a>
    <br>
    <em>CVPR</em>, 2024 &nbsp
    <br>
    <a href="https://yfeng95.github.io/ChatPose/">project page</a>
    /
    <a href="https://arxiv.org/abs/2311.18836">arXiv</a>
    /
    <a href="https://github.com/yfeng95/PoseGPT">code</a>
    <p></p>
    <p>
    ChatPose adopts LLMs to reason about SMPL-based regression from images or textual descriptions.
    </p>
  </td>
</tr>


<tr onmouseout="woc_stop()" onmouseover="woc_start()">
  <td class="paper_tb">
    <div class="tb_one">
      <div class="tb_two">
        <img src="images/woc.png">
      </div>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2209.00776">
      <papertitle>WOC: A Handy Webcam-based 3D Online Chatroom</papertitle>
    </a>
    <br>
    <a href="https://github.com/yanchxx">Chuanhang Yan*</a>,
    <strong>Yu Sun*</strong>,
    <a href="https://scholar.google.com/citations?user=vhM_c14AAAAJ&hl=zh-CN">Qian Bao</a>,
    <a href="https://ieeexplore.ieee.org/author/37088502890">Jinhui Pang</a>,
    <a href="https://drliuwu.com">Wu Liu</a>,
    <a href="https://taomei.me/">Tao Mei</a>
    <br>
    * equal contribution. 
    <br>
    <em>MM demo</em>, 2022 &nbsp
    <br>
    <a href="https://arxiv.org/abs/2209.00776">arXiv</a>
    /
    <a href="references/woc.bib">bib</a>
    <p></p>
    <p>
    WOC captures the 3D motion of users with a single camera and drives their individual 3D virtual avatars in real-time in an online chatroom.
    </p>
  </td>
</tr>


<tr onmouseout="woc_stop()" onmouseover="woc_start()">
  <td class="paper_tb">
    <div class="tb_one">
      <div class="tb_two">
        <img src="images/survey.png">
      </div>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://dl.acm.org/doi/abs/10.1145/3524497">
      <papertitle>Recent Advances of Monocular 2D and 3D Human Pose Estimation: A Deep Learning Perspective</papertitle>
    </a>
    <br>
    <a href="https://drliuwu.com">Wu Liu</a>,
    <a href="https://scholar.google.com/citations?user=vhM_c14AAAAJ&hl=zh-CN">Qian Bao</a>,
    <strong>Yu Sun</strong>,
    <a href="https://taomei.me/">Tao Mei</a>
    <br>
    <br>
    <em>ACM Computing Surveys (CSUR)</em>, 2022 &nbsp
    <br>
    <a href="https://arxiv.org/pdf/2104.11536.pdf">arXiv</a>
    /
    <a href="references/csur.bib">bib</a>
    <p></p>
    <p>
    A survey about monocular 2D and 3D human pose estimation. 
    </p>
  </td>
</tr>

<!-- </tbody></table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
  <tr>
    <td>
      <heading>Academic Services</heading>
    </td>
  </tr>
</tbody></table>
<table width="100%" align="center" border="0" cellpadding="20"><tbody>
  
  <tr>
    <td width="100%" valign="center">
      <strong>Conference Reviewer:</strong> CVPR22, ICCV21, 3DV20.
      <br>
      <strong>Journal Reviewer:</strong> Computers & Graphics.
      <br>
    </td>
  </tr> -->
  
</tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:0px">
      <br>
      <p style="text-align:right;font-size:small;">
        The template is borrowed from <a href="https://jonbarron.info/">this awesome website</a>
      </p>
    </td>
  </tr>
</tbody></table>
</td>
</tr>
</table>

</body>

</html>
