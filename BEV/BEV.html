<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
      <meta name="description" content="We introduce BEV, a monocular one-stage method with an efficient new “bird’s-eye-view” representation, which enables the network to explicitly reason about people in 3D.">
       <meta name="keywords" content="3D representation; relative depth reasoning; “bird’s-eye-view” representation; Relative Human dataset; all age groups;">
      <meta name="author" content="Yu Sun">
      <title>Putting People in their Place: Monocular Regression of 3D People in Depth</title>

      <!--<meta property="og:title" content="[CVPR 2022] BEV: ..." />-->
      <!--<meta property="og:description" content="BEV is a ... ">-->
      <!--<meta property="og:image" content="images/teaser.jpg" />-->

      <!--<meta name="twitter:card" content="summary_large_image" />-->
      <!--<meta name="twitter:title" content="[CVPR 2022] BEV: ..." />-->
      <!--<meta name="twitter:description" content="BEV is a ..." />-->
      <!--<meta name="twitter:image" content="https://neuralbodies.github.io/LEAP/images/teaser1200x630.jpg" />-->
      <!--<meta name="twitter:image:alt" content="BEV CVPR 2022" />-->

      <!-- Bootstrap core CSS -->
      <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
      <!-- Custom styles for this template -->
      <link href="css/scrolling-nav.css" rel="stylesheet">
      <!-- nice figures  -->
      <!--<link rel="stylesheet" href="css/font-awesome.css">-->
      <!--<link rel="icon" type="image/png" href="images/favicon.png">-->

   </head>
   <body id="page-top">
      <!-- Navigation -->
      <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
         <div class="container">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">BEV</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
               <ul class="navbar-nav ml-auto">
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#about">About</a>
                  </li>
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#overview">Overview</a>
                  </li>
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#demos">Demos</a>
                  </li>
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#citation">Citation</a>
                  </li>
               </ul>
            </div>
         </div>
      </nav>


      <header class="bg-light text-black">
          <div class="container text-center">
             <h1>BEV</h1>
             <h2>Putting People in their Place: <br /> Monocular Regression of 3D People in Depth</h2><br>
              <div id="content">
          <div id="content-inner">

            <div class="section head">

                <div class="authors">
                    <h3>
                        <a href="https://github.com/Arthur151">Yu Sun</a><sup>1,2</sup>&nbsp;
                        <a href="https://drliuwu.com/english.html">Wu Liu</a><sup>2</sup>&nbsp;
                        <a href="https://scholar.google.com/citations?user=vhM_c14AAAAJ&hl=zh-CN">Qian Bao</a><sup>2</sup>&nbsp;
                        <a href="http://homepage.hit.edu.cn/fuyili">Yili Fu</a><sup>1</sup>&nbsp;
                        <a href="https://taomei.me/">Tao Mei</a><sup>2</sup>
                        <a href="https://ps.is.mpg.de/~black">Michael J. Black</a><sup>3</sup>
                        <h3>
                </div>

                <div class="affiliations">
                    <h5>
                        <sup>1</sup><a href="http://en.hit.edu.cn/">Harbin Institute of Technology</a>&nbsp;
                        <sup>2</sup><a href="https://corporate.jd.com/">Explore Academy of JD.com, Beijing<br></a>
                        <sup>3</sup><a href="https://ps.is.mpg.de/research/">Max Planck Institute for Intelligent Systems, Tubingen<br></a>
                        <h5>
                </div>

                <div class="venue"><h4> IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) 2022<h5></div>

                <div class="downloads">
                    <br><h2>
                    <a class="publink" href="https://arxiv.org/abs/2112.08274" target="_blank" style="text-decoration: none"> Paper <i class="fa fa-print"></i></a>&nbsp;&nbsp;
                    <a class="publink" href="https://github.com/Arthur151/ROMP" target="_blank" style="text-decoration: none"> Code <i class="fa fa-github"></i></a> &nbsp;&nbsp;
                    <a class="publink" href="https://github.com/Arthur151/Relative_Human" target="_blank" style="text-decoration: none"> Dataset <i class="fa fa-github"></i></a> &nbsp;&nbsp;
                    
                    <h3>
                </div>
            </div>
        </div>
      </header>


      <section id="about" class="about-section">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                <div class="embed-responsive embed-responsive-16by9">
                    <iframe class="embed-responsive-item" src="https://youtu.be/Q62fj_6AxRI" title="BEV: Monocular Regression of Multiple 3D People in Depth (CVPR 2022)"
                        frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>
                <p class="lead text-justify">
                    Given an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their
                    relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their
                    height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to
                    adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple
                    people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane,
                    our method, called BEV, adds an additional imaginary Bird's-Eye-View representation to explicitly reason about depth.
                    BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position.
                    Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making
                    it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body
                    model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically,
                    we create a "Relative Human" (RH) dataset that includes age labels and relative depth relationships between the people
                    in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme.
                    BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code and
                    dataset are released for research purposes.
                </p>
               </div>
            </div>
         </div>
      </section>

    <section id="overview" class="">
        <div class="container">
            <div class="row">
                <div class="col-lg-10 mx-auto">
                    <h2>Overview</h2>
                    <div class="text-center">
                        <p>
                            <img class="img-fluid" alt="method overview" src="images/pipeline.png">
                        </p>
                    </div>
                    <p class="lead text-justify">
                        BEV adopts a multi-head architecture. Given a single RGB image as input, BEV outputs 5 maps. For coarse-to-fine localization,
                        we use the first 4 maps, which are the Body Center heatmaps and the Localization Offset maps in the front view and bird's-eye
                        view. We first expand the front-/bird's-eye-view maps in depth/height wise and then combine them to generate the 3D Center/Offset
                        maps. For coarse detection, we extract the rough 3D position of people from the 3D Center map. For fine localization,
                        we sample the offset vectors from the 3D Offset map at the corresponding 3D center position. Adding these gives the 3D
                        translation prediction. For 3D mesh parameter regression, we use the estimated 3D translation (x, y, d) and the Mesh
                        Feature map. The depth value d of 3D translation is mapped to a depth encoding. At (x, y), we sample a feature vector
                        from the Mesh Feature map and add it with the depth encoding for final parameter regression. Finally, we convert the
                        estimated parameters to body meshes using the SMPL+A model.
                    </p>
                </div>
            </div>
        </div>
    </section>

      <section id="citation" class="">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                  <h2 class="section-title-tc">Citation</h2>
<pre style="display: block; background-color: #f5f5f5; border: 1px solid #ccc; border-radius: 4px">
@InProceedings{BEV,
    author = {Sun, Yu and Liu, Wu and Bao, Qian and Fu, Yili and Mei, Tao and Black, Michael J.},
    title = {Putting People in their Place: Monocular Regression of 3D People in Depth}, 
    booktitle = {IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)}, 
    month = june, 
    year = {2022}}
</pre>
               </div>
            </div>
         </div>
      </section>


      <section id="contact" class="">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
	                <h2>Contact</h2>
		            <br>For questions or discussion, please contact Yu Sun: <a href="mailto:yusun@stu.hit.edu.cn">yusun@stu.hit.edu.cn</a>
               </div>
            </div>
         </div>
      </section>


      <!-- Footer -->
      <footer class="py-5 bg-dark">
      </footer>
      <!-- Bootstrap core JavaScript -->
      <script src="vendor/jquery/jquery.min.js"></script>
      <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
      <!-- Plugin JavaScript -->
      <script src="vendor/jquery-easing/jquery.easing.min.js"></script>
      <!-- Custom JavaScript for this theme -->
      <script src="js/scrolling-nav.js"></script>
   </body>
</html>
