<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
      <meta name="description" content="We introduce BEV, a monocular one-stage method with an efficient new “bird’s-eye-view” representation, which enables the network to explicitly reason about people in 3D.">
       <meta name="keywords" content="3D representation; relative depth reasoning; “bird’s-eye-view” representation; Relative Human dataset; all age groups;">
      <meta name="author" content="Yu Sun">
      <title>Putting People in their Place: Monocular Regression of 3D People in Depth</title>

      <!--<meta property="og:title" content="[CVPR 2022] BEV" />-->
      <!--<meta property="og:description" content="BEV is a ... ">-->
      <!--<meta property="og:image" content="images/teaser.jpg" />-->

      <!--<meta name="twitter:card" content="summary_large_image" />-->
      <!--<meta name="twitter:title" content="[CVPR 2022] BEV: Monocular Regression of 3D People in Depth" />-->
      <!--<meta name="twitter:description" content="BEV is a ..." />-->
      <!--<meta name="twitter:image" content="https://yusun.github.io/LEAP/images/teaser.jpg" />-->
      <!--<meta name="twitter:image:alt" content="BEV [CVPR22]" />-->

      <!-- Bootstrap core CSS -->
      <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
      <!-- Custom styles for this template -->
      <link href="css/scrolling-nav.css" rel="stylesheet">
      <!-- nice figures  -->
      <!--<link rel="stylesheet" href="css/font-awesome.css">-->
      <!--<link rel="icon" type="image/png" href="images/favicon.png">-->

   </head>
   <body id="page-top">
      <!-- Navigation -->
      <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
         <div class="container">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">BEV</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
               <ul class="navbar-nav ml-auto">
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#about">About</a>
                  </li>
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#overview">Overview</a>
                  </li>
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#demos">Demos</a>
                  </li>
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#citation">Citation</a>
                  </li>
               </ul>
            </div>
         </div>
      </nav>


      <header class="bg-light text-black">
          <div class="container text-center">
             <h1>BEV</h1>
             <h2>Putting People in their Place: <br /> Monocular Regression of 3D People in Depth</h2><br>
              <div id="content">
          <div id="content-inner">

            <div class="section head">

                <div class="authors">
                    <h3>
                        <a href="https://github.com/Arthur151">Yu Sun</a><sup>1,2</sup>&nbsp;
                        <a href="https://drliuwu.com/english.html">Wu Liu</a><sup>2</sup>&nbsp;
                        <a href="https://scholar.google.com/citations?user=vhM_c14AAAAJ&hl=zh-CN">Qian Bao</a><sup>2</sup>&nbsp;
                        <a href="http://homepage.hit.edu.cn/fuyili">Yili Fu</a><sup>1</sup>&nbsp;
                        <a href="https://taomei.me/">Tao Mei</a><sup>2</sup>
                        <a href="https://ps.is.mpg.de/~black">Michael J. Black</a><sup>3</sup>
                        <h3>
                </div>

                <div class="affiliations">
                    <h5>
                        <sup>1</sup><a href="http://en.hit.edu.cn/">Harbin Institute of Technology</a>&nbsp;
                        <sup>2</sup><a href="https://corporate.jd.com/">Explore Academy of JD.com, Beijing<br></a>
                        <sup>3</sup><a href="https://ps.is.mpg.de/research/">Max Planck Institute for Intelligent Systems, Tubingen<br></a>
                        <h5>
                </div>

                <div class="venue"><h4> IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) 2022<h5></div>

                <div class="downloads">
                    <br><h2>
                    <a class="publink" href="https://arxiv.org/abs/2112.08274" target="_blank" style="text-decoration: none"> Paper <i class="fa fa-print"></i></a>&nbsp;&nbsp;
                    <a class="publink" href="https://github.com/Arthur151/ROMP" target="_blank" style="text-decoration: none"> Code <i class="fa fa-github"></i></a> &nbsp;&nbsp;
                    <a class="publink" href="https://github.com/Arthur151/Relative_Human" target="_blank" style="text-decoration: none"> Dataset <i class="fa fa-github"></i></a> &nbsp;&nbsp;
                    
                    <h3>
                </div>
            </div>
        </div>
      </header>


      <section id="about" class="about-section">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                <div class="embed-responsive embed-responsive-16by9">
                    <iframe class="embed-responsive-item" src="https://www.youtube.com/watch?v=hunBPJxnyBU" title="BEV: Monocular Regression of Multiple 3D People in Depth (CVPR 2022)"
                        frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>
                <p class="text-justify">
                    Given an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their
                    relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their
                    height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to
                    adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple
                    people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane,
                    our method, called BEV, adds an additional imaginary Bird's-Eye-View representation to explicitly reason about depth.
                    BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position.
                    Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making
                    it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body
                    model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically,
                    we create a "Relative Human" (RH) dataset that includes age labels and relative depth relationships between the people
                    in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme.
                    BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code and
                    dataset are released for research purposes.
                </p>
               </div>
            </div>
         </div>
      </section>

    <section id="overview" class="">
        <div class="container">
            <div class="row">
                <div class="col-lg-10 mx-auto">
                    <h2>Overview</h2>
                    <div class="text-center">
                        <p>
                            <img class="img-fluid" alt="method overview" src="images/BEV_overview.png">
                        </p>
                    </div>
                    <p class="text-justify">
                        BEV adopts a multi-head architecture. Given a single RGB image as input, BEV outputs 5 maps. For coarse-to-fine localization,
                        we use the first 4 maps, which are the Body Center heatmaps and the Localization Offset maps in the front view and bird's-eye
                        view. We first expand the front-/bird's-eye-view maps in depth/height wise and then combine them to generate the 3D Center/Offset
                        maps. For coarse detection, we extract the rough 3D position of people from the 3D Center map. For fine localization,
                        we sample the offset vectors from the 3D Offset map at the corresponding 3D center position. Adding these gives the 3D
                        translation prediction. For 3D mesh parameter regression, we use the estimated 3D translation (x, y, d) and the Mesh
                        Feature map. The depth value d of 3D translation is mapped to a depth encoding. At (x, y), we sample a feature vector
                        from the Mesh Feature map and add it with the depth encoding for final parameter regression. Finally, we convert the
                        estimated parameters to body meshes using the SMPL+A model.
                    </p>
                    <div class="text-center">
                        <p>
                            <img class="img-fluid" alt="method overview" src="images/RH.png">
                        </p>
                        <p class="text-justify">
                        Existing in-the-wild datasets lack groups of overlapping people with annotations. Since acquiring 3D annotations of large
                        crowds is challenging, we exploit more cost-effective weak annotations. We collect a new dataset, named Relative Human (RH),
                        to support in-the-wild monocular human depth reasoning. As shown in the figure above, we annotate the relative depth relationship between all people in the image. We treat
                        subjects whose depth difference is less than one body-width as people in the same layer. We then classify
                        all people into different depth layers (DLs). Unlike prior work, which labels the ordinal relationships between pairs of
                        joints of individuals, DLs capture the depth order of multiple people. Additionally, we label people
                        with four age categories: adults, teenagers, children, and babies. In total, we collect about 7.6K images with weak annotations
                        of over 24.8K people. More than 21% of the subjects are young people (5.3K), including teenagers, children, and babies.
                        To learn from these weak annotations, we develop two relative loss functions, piece-wise depth layer loss
                        and ambiguity-compatible age loss.
                        For more details, please refer to <a href="https://arxiv.org/abs/2112.08274">our paper</a>.
                        
                        </a>
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section id="demos" class="about-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-10 mx-auto">
                    <h2>Demos</h2>
                    <div class="text-center">
                        <p>
                            <h4>Qualitative Comparisons to previous SOTAs.</h4>
                            <img class="img-fluid" alt="demo" src="images/compare1.png">
                            <img class="img-fluid" alt="demo" src="images/compare2.png">
                        </p>
                    </div>
                    <div class="text-center">
                        <p>
                        <h4>Qaulitative results on Internet images.</h4>
                        <img class="img-fluid" alt="demo" src="images/demo1.png">
                        <img class="img-fluid" alt="demo" src="images/crowd1.png" width="50%"><img class="img-fluid" alt="demo" src="images/standing_rotating.gif" width="50%">
                        <img class="img-fluid" alt="demo" src="images/crowd2.png" width="50%"><img class="img-fluid" alt="demo" src="images/conference_rotating.gif" width="50%">
                        <img class="img-fluid" alt="demo" src="images/crowd3.png" width="50%"><img class="img-fluid" alt="demo" src="images/tennis_rotating.gif" width="50%">
                    </p>
                    </div>
                </div>
            </div>
        </div>
    </section>


      <section id="citation" class="">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                  <h2 class="section-title-tc">Citation</h2>
<pre style="display: block; background-color: #f5f5f5; border: 1px solid #ccc; border-radius: 4px">
@InProceedings{BEV,
    author = {Sun, Yu and Liu, Wu and Bao, Qian and Fu, Yili and Mei, Tao and Black, Michael J.},
    title = {Putting People in their Place: Monocular Regression of 3D People in Depth}, 
    booktitle = {IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)}, 
    month = jun, 
    year = {2022}}
</pre>
               </div>
            </div>
         </div>
      </section>

    <section id="acknowledgement" class="">
        <div class="container">
            <div class="row">
                <div class="col-lg-10 mx-auto">
                    <h2>Acknowledgement</h2>
                    This work was supported by the National Key R&D Program of China under Grand No. 2020AAA0103800. <br />
                    <b>Disclosure</b>: MJB has received research funds from Adobe, Intel, Nvidia, Facebook, and Amazon and has financial interests in Amazon, Datagen Technologies, and Meshcapade GmbH. While he was part-time at Amazon during this project, his research was performed solely at Max Planck.
                    <br />This project page is modified from <a href="https://sanweiliti.github.io/LEMO/LEMO.html">this page</a>.
                </div>
            </div>
        </div>
    </section>


      <section id="contact" class="">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                    <h2>Contact</h2>
                    For questions or discussion, please contact Yu Sun: <a href="mailto:yusun@stu.hit.edu.cn">yusun@stu.hit.edu.cn</a>
               </div>
            </div>
         </div>
      </section>


      <!-- Footer -->
      <footer class="py-5 bg-dark">
      </footer>
      <!-- Bootstrap core JavaScript -->
      <script src="vendor/jquery/jquery.min.js"></script>
      <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
      <!-- Plugin JavaScript -->
      <script src="vendor/jquery-easing/jquery.easing.min.js"></script>
      <!-- Custom JavaScript for this theme -->
      <script src="js/scrolling-nav.js"></script>
   </body>
</html>
